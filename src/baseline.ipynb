{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot\n",
    "\n",
    "import yaml \n",
    "\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "import lightning as L\n",
    "from pytorch_lightning.loggers import WandbLogger"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# test\n",
    "backbone_weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "preprocessor = backbone_weights.transforms()\n",
    "backbone = torchvision.models.resnet34(weights=backbone_weights)"
   ],
   "id": "ece694fe06b80972",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with open(\"parameters.yaml\", \"r\") as yaml_file:\n",
    "    parameters = yaml.load(yaml_file, Loader=yaml.FullLoader)\n",
    "from pprint import pprint\n",
    "pprint(parameters)"
   ],
   "id": "5e3703dc867ad4cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# fully-supervised fine-tuning\n",
    "class Backbone(L.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_classes, user_parameters):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_classes = n_classes\n",
    "        self.user_parameters = user_parameters\n",
    "        \n",
    "        self.backbone_weights = torchvision.models.ResNet34_Weights.DEFAULT\n",
    "        self.preprocessor = self.backbone_weights.transforms()\n",
    "        self.backbone = torchvision.models.resnet34(weights=self.backbone_weights)\n",
    "        self.n_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "        self.head = nn.Linear(self.n_features, self.n_classes)\n",
    "        \n",
    "        match self.user_parameters['Loss_Function']['loss_function']:\n",
    "            case 'cross_entropy':\n",
    "                self.loss_function = F.cross_entropy\n",
    "            case 'qwk':\n",
    "                from WeightedKappaLoss import WeightedKappaLoss\n",
    "                self.loss_function = WeightedKappaLoss(self.n_classes, mode='quadratic')\n",
    "            case _:\n",
    "                self.loss_function = F.cross_entropy  # defaults to cross entropy\n",
    "\n",
    "        self.save_hyperparameters()  # wandb\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_processed = self.preprocessor(x)\n",
    "        x_features = self.backbone(x_processed)\n",
    "        return self.head(x_features)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            [\n",
    "                {\"params\": self.backbone.parameters(),\n",
    "                 \"name\": \"backbone\"},\n",
    "                {\"params\": self.head.parameters(),\n",
    "                 \"name\": \"head\"},\n",
    "            ],  \n",
    "            lr=self.user_parameters['Optimizer']['lr'],\n",
    "            weight_decay=self.user_parameters['Optimizer']['weight_decay'],\n",
    "        )\n",
    "        \n",
    "        return optimizer\n",
    "        \n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        x, y = train_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        y_hat = probas.argmax(dim=1)\n",
    "        \n",
    "        accuracy = torchmetrics.functional.accuracy(\n",
    "            y_hat, y, task='multiclass', num_classes=self.n_classes\n",
    "        )\n",
    "        \n",
    "        qwk = torchmetrics.functional.cohen_kappa(\n",
    "            y_hat, y, task='multiclass', num_classes=self.n_classes,\n",
    "            weights='quadratic'\n",
    "        )   \n",
    "        \n",
    "        recall = torchmetrics.functional.recall(\n",
    "            y_hat, y, task='multiclass', num_classes=self.n_classes,\n",
    "        )\n",
    "        \n",
    "        self.log(\"train/loss\", loss)  # wandb\n",
    "        self.log(\"train/accuracy\", accuracy)  # wandb\n",
    "        self.log(\"train/recall\", recall)  # wandb\n",
    "        self.log(\"train/qwk\", qwk)  # wandb\n",
    "        \n",
    "        self.log(\"trainable_parameters\", sum([p.numel() for p in self.parameters() if p.requires_grad]))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        x, y = val_batch\n",
    "        logits = self.forward(x)\n",
    "        loss = F.cross_entropy(logits, y)\n",
    "        \n",
    "        probas = F.softmax(logits, dim=1)\n",
    "        y_hat = probas.argmax(dim=1)\n",
    "        \n",
    "        accuracy = torchmetrics.functional.accuracy(\n",
    "            y_hat, y, task='multiclass', num_classes=self.n_classes\n",
    "        )\n",
    "        \n",
    "        qwk = torchmetrics.functional.cohen_kappa(\n",
    "            y_hat, y, task='multiclass', num_classes=self.n_classes,\n",
    "            weights='quadratic'\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.log(\"val/loss\", loss)  # wandb\n",
    "        self.log(\"val/accuracy\", accuracy)  # wandb\n",
    "        self.log(\"val/qwk\", qwk)  # wandb\n",
    "\n",
    "    \n",
    "        "
   ],
   "id": "5bb9fe1b01bae0e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model = Backbone(n_classes=3, user_parameters=parameters)",
   "id": "92e148e98b0102e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project='debug-runs',\n",
    "    config=parameters,\n",
    ")"
   ],
   "id": "634b231e358d71a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fake_train_data = torchvision.datasets.FakeData(\n",
    "    size=1000,\n",
    "    image_size=(3, 512, 512),\n",
    "    num_classes=3,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "fake_val_data = torchvision.datasets.FakeData(\n",
    "    size=100,\n",
    "    image_size=(3, 512, 512),\n",
    "    num_classes=3,\n",
    "    transform=torchvision.transforms.ToTensor()\n",
    ")\n",
    "\n",
    "fake_train_data"
   ],
   "id": "945419bcf58bb6ef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fake_train_dataloader = torch.utils.data.DataLoader(\n",
    "    batch_size=parameters['Training']['batch_size'],\n",
    "    dataset=fake_train_data,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "fake_val_dataloader = torch.utils.data.DataLoader(\n",
    "    batch_size=parameters['Training']['batch_size'],\n",
    "    dataset=fake_val_data,\n",
    "    num_workers=4,\n",
    ")"
   ],
   "id": "c56654edc59e3ea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "callback_model_checkpoint = L.pytorch.callbacks.ModelCheckpoint(\n",
    "    dirpath='checkpoints',\n",
    "    filename='{epoch}',\n",
    "    monitor='val/loss',\n",
    "    save_last=True,\n",
    "    save_top_k=1,\n",
    ")\n",
    "\n",
    "callback_early_stopping = L.pytorch.callbacks.EarlyStopping(\n",
    "    monitor='val/loss',\n",
    "    patience=parameters['Training']['early_stopping_patience'],\n",
    ")\n",
    "\n",
    "callback_backbone_finetuning = L.pytorch.callbacks.BackboneFinetuning(\n",
    "    unfreeze_backbone_at_epoch=parameters['Training']['epochs_before_unfreeze'],\n",
    "    lambda_func=lambda lr: parameters['Training']['gain_after_unfreeze'],\n",
    "    backbone_initial_ratio_lr=parameters['Training']['gain_before_unfreeze'],\n",
    ")\n",
    "\n",
    "callback_learning_rate_monitor = L.pytorch.callbacks.LearningRateMonitor(logging_interval='step')\n",
    "\n",
    "callbacks = [\n",
    "    # callback_model_checkpoint,\n",
    "    # callback_early_stopping,\n",
    "    callback_backbone_finetuning,\n",
    "    callback_learning_rate_monitor,\n",
    "]"
   ],
   "id": "3e3b76956eeafaba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model = Backbone(n_classes=3, user_parameters=parameters)\n",
    "trainer = L.Trainer(\n",
    "    limit_train_batches=parameters['Training']['limit_train_batches'], \n",
    "    max_epochs=parameters['Training']['max_epochs'],\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    log_every_n_steps=parameters['Training']['log_every_n_steps'],\n",
    ")"
   ],
   "id": "e407530417b98dd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "trainer.fit(model, fake_train_dataloader, fake_val_dataloader)\n",
    "wandb.finish()"
   ],
   "id": "4ee93e3fa9e5a13a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "c6935e04a34c895c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "cc0debd363caae15",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
